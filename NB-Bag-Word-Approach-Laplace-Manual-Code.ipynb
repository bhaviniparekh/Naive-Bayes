{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "\n",
    "This code is to understand the working of Naive Bayes classifier with text/document type of data. The text data requires special \n",
    "approach before it can be presented to the predictive model. This code implements a concept called Bag of Word. In this approach\n",
    "data frame rows containing text/document are converted into a list of distinct words.This distinct words then are \n",
    "transformed into a feature in a new data frame. The frequency/count of these distinct words in every row of text/document data\n",
    "frame becomes values for corresponding rows in the newly transformed data frame. Once a new data frame is ready with distinct\n",
    "words as column and frequency of those words as row, it is now ready for probability calculation. While calculating probability\n",
    "Laplace smoothing is implemented to avoid 0 probability. Thereafter validation data is run over the training model to predicate\n",
    "a label for each row. The accuracy score is calculated at the end.\n",
    "\n",
    "The details steps are as below.\n",
    "\n",
    "The text/document data is read using the pandas read_csv function and converted into a data frame. Each row of the data frame \n",
    "represents a text/document. The data is split to get input(X) and output(Y) data. The X data is train-test split into training\n",
    "and validation data.\n",
    "\n",
    "The rows of the training data frame are split to create a list of words. From this list, a set of distinct words is created. \n",
    "A new training data frame is created with distinct words as a feature. Each row in this new data frame will correspond to row\n",
    "in the original data frame. To populate the row, the original data frame is iterated through rows. Each row is split into words,\n",
    "an incremental count for that word is populated in the corresponding row, column(word) location.\n",
    "This way training data is converted into words based feature data frame.\n",
    "\n",
    "\n",
    "For validation data, a similar data frame is created with training data's distinct words as features. To populate this new data\n",
    "frame, the validation data is iterated through rows. Each row is split into words, an incremental count for that word is populated\n",
    "in the corresponding row, column(word) location. This way new data frame for validation data is ready.\n",
    "\n",
    "\n",
    "For probability, the probability of each unique class of Y data is calculated and updated in a dictionary object.\n",
    "Thereafter a training model is built, for this conditional probability of training data is calculated. For each of the distinct\n",
    "words of the training, data set the corresponding sum of the count of matching distinct word given unique class  + 1/total count\n",
    "of that distinct word +len(new data frame) is calculated for the new data frame.\n",
    "eg The| no sport or game|sport, etc.\n",
    "This process is done for every unique value of the class. The dictionary object is populated.\n",
    "eg The{Thesport:probability,Thenosport:probabilty}\n",
    " \n",
    "For prediction, validation data is iterated through rows, corresponding to each row, the respective column name is matched with\n",
    "the model's probability dictionary ie column | unique class (eg given above). For each row, for all columns in that row,\n",
    "column|class probability is multiplied and cumulative probability is finally multiplied with the probability of that class\n",
    "and updated in the dictionary (class: total probability).\n",
    "This process is repeated for all unique values. Thus every row will have as many unique values that many probabilities.\n",
    "The class: probability is populated in a dictionary.\n",
    "The class which has the highest probability will be predicted for that row. This way a list is\n",
    "populated with the prediction for validation data.\n",
    " \n",
    "Finally, the accuracy score is calculated with predicted data and actual Y data.\n",
    "\n",
    "\n",
    "\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    '''\n",
    "   This function is to read data using Pandas read_csv function and convert into dataframe.\n",
    "    \n",
    "    Return: - Dataframe     \n",
    "    '''\n",
    "    \n",
    "    data=pd.read_csv('D:\\Dataset\\Bag-Word-Data.csv',header=None,names=['sentences','category'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def data_preprocess(df):\n",
    "    '''\n",
    "    This function is to split data into X and Y(input and output data)using the data frame.\n",
    "    \n",
    "    The X data is further train-test-Split to get training and testing data\n",
    "    \n",
    "    Argument :-\n",
    "    df   :- Dataframe with text/document data.    \n",
    "    \n",
    "    Return :- \n",
    "    x_train:-training data\n",
    "    x_test :- Validation data\n",
    "    y_train :- Label data of training data\n",
    "    y_test :- label data of validation data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    x = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    #spliting the dataset into training and test set\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y,test_size=.3, random_state=0)\n",
    "    \n",
    "    return xtrain,xtest,ytrain,ytest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "def list_of_word(datatoget):\n",
    "    \n",
    "    '''\n",
    "    This function makes a list of sentences for every row of the data frame. Then it splits the sentences to make a list of words.\n",
    "    From this list of words, it creates a set of distinct words.\n",
    "    This process is done for both the training and validation data set.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Argument :-\n",
    "    datatoget :- Text/document based data frame.\n",
    "    \n",
    "    Return :- \n",
    "    dist_word : list of distinct words\n",
    "    \n",
    "    listword :- List of words.\n",
    "    \n",
    "    '''\n",
    "    listword=[]\n",
    "    commonword=[]\n",
    "    dist_word=[]\n",
    "    \n",
    "    listword=datatoget['sentences'].values.tolist()\n",
    "    \n",
    "    \n",
    "    #Create a single list of words from the above snetences FOR X_train.Once the list is made,get distinct words from the list\n",
    "\n",
    "    for i in range(len(listword)):\n",
    "        words=[]\n",
    "        words=listword[i].split()\n",
    "        \n",
    "        for j in range(len(words)):\n",
    "               commonword.append(words[j].lower())\n",
    "        words.clear()\n",
    "\n",
    "    dist_word=list(set(commonword))\n",
    "\n",
    "    \n",
    "    return dist_word,listword\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_distinct_word_df(train_list_word,x_train,train_dist_word):\n",
    "    \n",
    "    '''\n",
    "    This function is to create a new data frame for training /validation data. A new data frame is created where distinct words\n",
    "    of training, data will be features. The same features will also be used to create a new data frame for validation data.\n",
    "    \n",
    "    The row index will correspond to training/validation data. The list of sentences of training data is iterated. Each sentence is \n",
    "    split into words. The words are incrementally counted and populated in the new data frame at the location of the index \n",
    "    value of the row and column =word since column name = distinct words.\n",
    "    \n",
    "    Argument :-\n",
    "    train_list_word :- List of sentences of training data.\n",
    "    x_train         :- Training data set\n",
    "    train_dist_word :- List of distinct words of training data set\n",
    "    \n",
    "    Return :-\n",
    "    newdf :- new data frame created.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Creating a dimension for new dataframe ,rows = number of rows in training data,columns = distinct words on training data.\n",
    "\n",
    "    zero_data = np.zeros(shape=(x_train.shape[0],len(train_dist_word)))\n",
    "    \n",
    "    #Creating data frame where column name = distinct words of the training dataset. Rows are populated with zero values.\n",
    "    newdf=pd.DataFrame(zero_data,columns=list(train_dist_word))\n",
    "  \n",
    "   \n",
    "\n",
    "    for i in range(len(train_list_word)):\n",
    "     \n",
    "        traincnt=[]\n",
    "        traincnt=train_list_word[i].split()\n",
    "        \n",
    "        \n",
    "        for j in traincnt:\n",
    "            j=j.lower()\n",
    "        \n",
    "            newdf.at[i,j]=newdf.at[i,j]+1\n",
    "               \n",
    "        traincnt.clear()\n",
    "    return newdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def class_prob(new_df,y_train):\n",
    "    \n",
    "    '''\n",
    "    The function is to calculate the probability of the unique value of Y data.\n",
    "    \n",
    "    Argument :- \n",
    "    newdf :- New data frame with distinct words as features\n",
    "    \n",
    "    y_train :-Ytrain data\n",
    "    \n",
    "    Return :- \n",
    "    outdict  :- Dictionary of unique class label:prob\n",
    "    unq_out  :- List of unique values of Y data\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #Find unique value of Output column and rowcount of X_train data \n",
    "    unq_out=y_train.unique()\n",
    "    rowcnt=y_train.shape[0]\n",
    "    outdict={}\n",
    "    \n",
    "     #concatenate new_df and y_train data for looping through column/name pair of input and output colmun,\n",
    "     #so have same index locations\n",
    "    concatdftrain=pd.concat([new_df,y_train],axis=1)\n",
    "    \n",
    "    # Getting probablity of ouput column for unqiue values\n",
    "    for k in unq_out:\n",
    "        prob=concatdftrain[concatdftrain[y_train.name]==k].shape[0]/rowcnt\n",
    "        outdict.update({k:prob})\n",
    "\n",
    "    return outdict,unq_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def prob_inp_out(train_dist_word,y_train,new_df,unqout):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "  This function is to calculate the probability of distinct words given a unique class of data. The same step is repeated for very\n",
    "    unique value of the Y data eg distinct_word|class 1,distincet_word|class 2..etc.This way for a given distinct word probabilities\n",
    "    with all unique classes are stored in a dictionary object.Training model is built.\n",
    "    \n",
    "    Argument :-\n",
    "    train_dist_word  :- distinct words of training data\n",
    "    y_train          :- Y data \n",
    "    new_df           :- new data frame with features as distinct words\n",
    "    unqout           :- List of unique values of Y data.\n",
    "    \n",
    "    Return :-\n",
    "    finalans :- Dictionary holding probabilities of every distinct word with every unique value of Y data.\n",
    "    \n",
    "    '''\n",
    "    prob=0\n",
    "    result=0\n",
    "    val=''\n",
    "    w=''\n",
    "    k=0\n",
    "    finalans={}\n",
    "    #concatenate new_df and y_train data for looping through column/name pair of input and output colmun ,\n",
    "    #so as to have same index locations.\n",
    "  \n",
    "    concatdftrain=pd.concat([new_df,y_train],axis=1)\n",
    "\n",
    "    for w in train_dist_word:\n",
    "        w=w.lower()\n",
    "        newdict={}\n",
    "        \n",
    "    \n",
    "        for val in unqout:\n",
    "            wordcount=0\n",
    "        \n",
    "            wordcount=concatdftrain[concatdftrain[y_train.name]==val][w].sum() +1\n",
    "        \n",
    "            prob=wordcount/(concatdftrain[concatdftrain[y_train.name]==val].iloc[:,:-1].sum().sum()+len(new_df.columns))\n",
    "            \n",
    "                   \n",
    "            newdict.update({w+val:prob}) #thenosport:probabilty\n",
    "        \n",
    "                \n",
    "        finalans.update({w:newdict})    #the:{thenosport:probabilty,thesport:probabilty}\n",
    "    return finalans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_dist_word_df(x_test,train_dist_word,test_list_word):\n",
    "    \n",
    "    '''\n",
    "    The function is to fit the validation data over distinct words of training data ie a new data frame is created with\n",
    "    distinct words of training data as features. The list of validation sentences is iterated.Each sentence is split into \n",
    "    words.If the words match the distinct words of training data then the row of the new validation data frame is populated\n",
    "    with an incremental count of the word. The word is added at the location of row index and column name=word.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Arguement :- \n",
    "    x_test  :- validation dataset\n",
    "    train_dist_word :- list of distinct words of training data set.\n",
    "    test_list_word :- List of sentences of validation data.\n",
    "    \n",
    "    Return :- \n",
    "    testnewdf :- new validation data frame.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Creating a matrix for new dataframe for X-Test data row=size of validation data ,columns = list of distinct words of training dataset\n",
    "    testzero_data = np.zeros(shape=(x_test.shape[0],len(train_dist_word)))\n",
    "    \n",
    "    #Creating dataframe with size=nos of sentence of X-data,columns=distinct words of X-train sentences\n",
    "    testnewdf=pd.DataFrame(testzero_data,columns=list(train_dist_word))\n",
    "       \n",
    "    \n",
    "\n",
    "    for tword in range(len(test_list_word)):\n",
    "    \n",
    "   \n",
    "        testcnt=[]\n",
    "        testcnt=test_list_word[tword].split()\n",
    "    \n",
    "        \n",
    "        for tcnt in testcnt:\n",
    "            tcnt=tcnt.lower()\n",
    "        \n",
    "            for trainw in train_dist_word:\n",
    "            \n",
    "                if tcnt==trainw:\n",
    "        \n",
    "                    testnewdf.at[tword,trainw]=testnewdf.at[tword,trainw]+1\n",
    "                else:\n",
    "                    pass\n",
    "           \n",
    "    \n",
    "                \n",
    "        testcnt.clear()\n",
    "    \n",
    "    return testnewdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def finalypred(test_new_df,inp_out_prob,unqout,class_dict):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    The function is created to predict validation data. The validation data is iterated overs rows.For each row, its iterated over\n",
    "    columns.For every column | unique class, the corresponding probability is obtained from the model dictionary of probabilities.\n",
    "    This way probabilites of all columns of a given row are multiplied and finally the cumulative column probabilty\n",
    "    is multiplied with probabilty of that class . \n",
    "    Similarly, this is followed for every unique value of Y data. Thus every row will have as many unique values that many probabilties.\n",
    "    Whichever class has the maximum probability is the predication for that row.\n",
    "    This way a list of predictions is obtained for validation data.\n",
    "    \n",
    "    Arguement :-\n",
    "    test_new_df  :- new validation data frame.\n",
    "    inp_out      : model dictionary with distinct word | class probabilities.\n",
    "    unqout   :- list of unique values of Y data.\n",
    "    class_dict  :- Dictinary of probabilities if every unique value of Y data.\n",
    "    \n",
    "    new test df,input-output column probabilty,class unique label,class unique label probabilty\n",
    "    \n",
    "    Return :- \n",
    "    Ypred  :- Array of predicted values.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    p=0\n",
    "    d={}\n",
    "    finalprob={}\n",
    "    testprob=1\n",
    "    Ypred=[]\n",
    "    \n",
    "\n",
    "# Iterating every row in x_test data\n",
    "    for index,row in test_new_df.iterrows():\n",
    "        \n",
    "  \n",
    "        # Iterating through unqiue values of Output column\n",
    "        for testval in unqout:\n",
    "            testprob=1\n",
    "\n",
    "            \n",
    "            #Iterating through columns of validation data.\n",
    "            for testcol in test_new_df.columns:\n",
    "                \n",
    "                \n",
    "                if row[testcol] >0:\n",
    "                    d=inp_out_prob[testcol]\n",
    "                    \n",
    "                \n",
    "                    p=d[testcol+testval]  # column|class probabilty is obtained from model's dictionary of probabilites \n",
    "                                          # eg the|sport or the|nosport\n",
    "                    \n",
    "                    \n",
    "                testprob=testprob*p \n",
    "                    \n",
    "                #eg RednY * SportnY *domesticnY\n",
    "        \n",
    "        \n",
    "            testprob=testprob*class_dict[testval] #Multiple cumulative column|class probability with that class probability\n",
    "            \n",
    "            finalprob.update({testval:testprob})\n",
    "        \n",
    "        max_v = max(zip(finalprob.values(), finalprob.keys()))  # find class having maximum probabilty\n",
    "    \n",
    "        Ypred.append(max_v[1]) # dictionary to populate prediction of a given row\n",
    "    return Ypred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracyfunc(ypred,y_test):\n",
    "    \n",
    "    '''\n",
    "    The function is to calculate the accuracy score with predicted data and actual labels.\n",
    "    \n",
    "    Arguement :- \n",
    "    ypred :- Prediction of validation data\n",
    "    y_test :- Actual class of the data set\n",
    "    \n",
    "    Return :- \n",
    "    accuracy :- Accuracy score\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ytest=y_test.tolist()\n",
    "\n",
    "    ytestrow=len(ytest)\n",
    "\n",
    "\n",
    "    cntrow=0\n",
    "    accuracy=0\n",
    "    \n",
    "    for x,y in zip(ytest,ypred):\n",
    "        \n",
    "        if x==y:\n",
    "            cntrow=cntrow+1\n",
    "  \n",
    "    accuracy=cntrow*100/ytestrow\n",
    "    print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data in pandas data frame\n",
    "df=read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split X and Y data.\n",
    "x_train,x_test,y_train,y_test = data_preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of distinct words and list of words of training data set\n",
    "train_dist_word,train_list_word=list_of_word(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of words of test data.\n",
    "test_dist_word,test_list_word=list_of_word(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new data frame with distinct words of training data as features and incremental count of distinct words as row value.\n",
    "new_df=train_distinct_word_df(train_list_word,x_train,train_dist_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create probability of every unique class of Y data.\n",
    "class_dict,unqout=class_prob(new_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate prabability of new training data set\n",
    "inp_out_prob=prob_inp_out(train_dist_word,y_train,new_df,unqout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new test dataframe with training distinct words as features and incremental count of test data words as row value.\n",
    "test_new_df=test_dist_word_df(x_test,train_dist_word,test_list_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict new test data over training model\n",
    "ypred=finalypred(test_new_df,inp_out_prob,unqout,class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.333333333333336\n"
     ]
    }
   ],
   "source": [
    "#Find accuracy score\n",
    "accuracyfunc(ypred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
