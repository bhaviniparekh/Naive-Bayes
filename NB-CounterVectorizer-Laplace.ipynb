{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code is to understand the working of Naive Bayes classifier with text/document type of data. The text data requires a \n",
    "special approach before it can be presented to the predictive model This code implements a concept called Bag of Words. In this\n",
    "approach data frame rows containing text/document are converted into a list of distinct words that are transformed into a\n",
    "feature in a new data frame. The frequency/count of these distinct words for a  row of text/document in the original data frame \n",
    "will become values in the newly transformed data frame. A new data frame is ready with distinct words as column and frequency\n",
    "of those words as a row for further steps. This whole process of converting text data to distinct words and get a count of \n",
    "these words in a given sentence/document will be implemented by the scikit library.\n",
    "\n",
    "In this code, we will implement the scikit library, CounterVectorizer.\n",
    "\n",
    "The details steps are as below.\n",
    "\n",
    "The text/document data is read using the pandas read_csv function and converted into a data frame. Each row of the data frame \n",
    "represents a text/document. The data is split to get input(X) and output(Y) data. The X data is train-test split into training\n",
    "and validation data.\n",
    "\n",
    "An object of CounterVectorizer is created.The training data is fit on this object. The fit method will learn the vocabulary\n",
    "and get distinct list of words. After fit, the data is transformed into an array of the count of the distinct words for each\n",
    "sentencs/document.The library also has an attribute to get the feature names ie the list of distinct words. The same process is\n",
    "done for validation data. Validation data is transformed based on training data ie the count of words for every sentence/document\n",
    "in the validation data is based on distinct words of the training data set. The end output of the transform step is an array of\n",
    "the count of the word for each row of training/validation data set and feature names(list of distinct words from training data\n",
    "set). \n",
    "The array is then converted into a data frame with columns based on distinct words of the training data set.\n",
    "\n",
    "For probability, the probability of each unique class of Y data is calculated and updated in a dictionary object.\n",
    "Thereafter a training model is built, for this conditional probability of training data is calculated with below formula\n",
    "\n",
    "(For each of the distinct words of the training data the corresponding sum of the count of that word given unique class) + 1\n",
    "__________________________________________________________________________________________________________________________\n",
    "                         total count of that distinct word +len(new data frame.columns) \n",
    "                         \n",
    "The Laplace smoothing is used in this formula to avoid 0 probability.\n",
    "\n",
    "eg The| no sport or game|sport, etc.\n",
    "This process is done for every unique value of the Y data. The dictionary object is populated as given in example below.\n",
    "eg The{Thesport:probability,Thenosport:probabilty}\n",
    " \n",
    "For prediction, validation data is iterated through rows, corresponding to each row, the respective column name is matched with\n",
    "the model probability dictionary ie column | unique class (eg given above).\n",
    "For each row, for all columns in that row, column|class probability is multiplied and cumulative probability is finally\n",
    "multiplied with probability of that class and updated in the dictionary (class: total probability). This process is repeated\n",
    "for all unique values. Thus every row will have as many unique values that many probabilities. All of class:probability are \n",
    "populated in a dictionary/.The class which has the highest probability will be predicted for that row. This way a list is\n",
    "populated with the prediction for validation data.\n",
    " \n",
    "Finally, the accuracy score is calculated with predicted data and actual Y data.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    '''\n",
    "    This function is to read data using Pandas read_csv function and convert into dataframe.\n",
    "    \n",
    "    Return: - \n",
    "    data :-Dataframe  holding the data.  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    data=pd.read_csv('D:\\Dataset\\Bag-Word-Data.csv',header=None,names=['sentences','category'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_preprocess(df):\n",
    "    '''\n",
    "    This function is to split data into X and Y(input and output data)using the data frame.\n",
    "    \n",
    "    The X data is further train-test-Split to get training and testing data\n",
    "    \n",
    "    Argument :-\n",
    "    df   :- Dataframe with text/document data.    \n",
    "    \n",
    "    Return :- \n",
    "    x_train:-training data\n",
    "    x_test :- Validation data\n",
    "    y_train :- Label data of training data\n",
    "    y_test :- label data of validation data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    x = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    #spliting the dataset into training and test.\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y,test_size=.3, random_state=0)\n",
    "    \n",
    "    return xtrain,xtest,ytrain,ytest\n",
    "\n",
    "\n",
    "\n",
    "def count_vector(datatofit,datatotransform):\n",
    "    '''\n",
    "    Create an object of CounterVectorizer.Fit the training data on this object. The fit will learn the vocabulary and get a\n",
    "    distinct list of words. After the fit, the data is transformed into an array of the count of the distinct words \n",
    "    in the sentences/document.The library also has an attribute to get the feature names ie the list of distinct words.\n",
    "    \n",
    "    \n",
    "    Argument :- \n",
    "    datatofit       :-training data \n",
    "    datatotransform :-testing data\n",
    "    \n",
    "    Return :-\n",
    "    datatotransform  :-tranformed training/validation data,t\n",
    "    data_dist        :- List of distinct words of training data set.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(datatofit.sentences)\n",
    "    datatotransform=vectorizer.transform(datatotransform.sentences).toarray()\n",
    "    data_dist=vectorizer.get_feature_names()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return datatotransform,data_dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_newdf(transformed_data,train_dist_word):\n",
    "    \n",
    "    '''\n",
    "    Create a new dataframe ,wheer column =training distinct word and row=training/Test transformed data.\n",
    "    \n",
    "    Argument :- Transformed data\n",
    "    \n",
    "    Return : New dataframe with transformed data.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    newdf=pd.DataFrame(transformed_data,columns=train_dist_word)\n",
    "    \n",
    "    return newdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def class_prob(new_df,y_train):\n",
    "    \n",
    "    '''\n",
    "    The function is to calculate the probability of the unique value of Y data.\n",
    "    \n",
    "    Argument :- \n",
    "    newdf :- New data frame with distinct words as features\n",
    "    \n",
    "    y_train :-Ytrain data\n",
    "    \n",
    "    Return :- \n",
    "    outdict  :- Dictionary of unique class label:prob\n",
    "    unq_out  :- List of unique values of Y data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Find unique value of Output column and rowcount of X_train data \n",
    "    unq_out=y_train.unique()\n",
    "    rowcnt=y_train.shape[0]\n",
    "    outdict={}\n",
    "    \n",
    "     #concatenate new_df and y_train data for looping through column/name pair of input and output colmun\n",
    "    #so have same index locations\n",
    "    concatdftrain=pd.concat([new_df,y_train],axis=1)\n",
    "    \n",
    "    # Getting probablity of ouput column|unqiue values\n",
    "    for k in unq_out:\n",
    "        prob=concatdftrain[concatdftrain[y_train.name]==k].shape[0]/rowcnt\n",
    "        outdict.update({k:prob})\n",
    "\n",
    "    return outdict,unq_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prob_inp_out(train_dist_word,y_train,new_df,unqout):\n",
    "    \n",
    "    '''\n",
    "    This function is to calculate the probability of distinct words given a unique class of data. The same step is repeated for very\n",
    "    unique value of the Y data eg distinct_word|class 1,distincet_word|class 2..etc.This way for a given distinct word probabilities\n",
    "    with all unique classes are stored in a dictionary object.Training model is built.\n",
    "    \n",
    "    Argument :-\n",
    "    train_dist_word  :- distinct words of training data\n",
    "    y_train          :- Y data \n",
    "    new_df           :- new data frame with features as distince words\n",
    "    unqout           :- List of unique values of Y data.\n",
    "    \n",
    "    Return :-\n",
    "    finalans :- Dictionary holding probabilities of every distinct word with every unique value of Y data.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # \n",
    "    finalans={}\n",
    "    inpdict={}\n",
    "\n",
    "    prob=0\n",
    "    result=0\n",
    "    val=''\n",
    "    w=''\n",
    "    k=0\n",
    " \n",
    "    #concatenate new_df and y_train data for looping through column/name pair of input and output colmum\n",
    "    #so have same index locations.\n",
    "    concatdftrain=pd.concat([new_df,y_train],axis=1)\n",
    "\n",
    "    for w in train_dist_word:\n",
    "        w=w.lower()\n",
    "        newdict={}\n",
    "        \n",
    "    \n",
    "        for val in unqout:\n",
    "            wordcount=0\n",
    "        \n",
    "            wordcount=concatdftrain[concatdftrain[y_train.name]==val][w].sum() +1\n",
    "        \n",
    "            prob=wordcount/(concatdftrain[concatdftrain[y_train.name]==val].iloc[:,:-1].sum().sum()+len(new_df.columns))\n",
    "            \n",
    "                   \n",
    "            newdict.update({w+val:prob})\n",
    "        \n",
    "                \n",
    "        finalans.update({w:newdict})\n",
    "    return finalans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def finalypred(test_new_df,inp_out_prob,unqout,class_dict):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    The function is created to predict validation data. The validation data is iterated overs rows.For each row, its iterated over\n",
    "    columns.For every column | unique class, the corresponding probability is obtained from the model dictionary of probabilities.\n",
    "    This way probabilites of all columns of a given row are multiplied and finally the cumulative column probabilty\n",
    "    is multiplied with probabilty of that class . \n",
    "    Similarly, this is followed for every unique value of Y data. Thus every row will have as many unique values that many probabilties.\n",
    "    Whichever class has the maximum probability is the predication for that row.\n",
    "    This way a list of predictions is obtained for validation data.\n",
    "    \n",
    "    Argument :-\n",
    "    test_new_df  :- new validation data frame.\n",
    "    inp_out      : model dictionary with distinct word | class probabilities.\n",
    "    unqout   :- list of unique values of Y data.\n",
    "    class_dict  :- Dictinary of probabilities if every unique value of Y data.\n",
    "    \n",
    "    new test df,input-output column probabilty,class unique label,class unique label probabilty\n",
    "    \n",
    "    Return :- \n",
    "    Ypred  :- Array of predicted values.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    p=0\n",
    "    d={}\n",
    "    finalprob={}\n",
    "    testprob=1\n",
    "    Ypred=[]\n",
    "    \n",
    "\n",
    "# Iterating every row in x_test data\n",
    "    for index,row in test_new_df.iterrows():\n",
    "        \n",
    "  \n",
    "        # Iterating through unqiue values of Output column\n",
    "        for testval in unqout:\n",
    "            testprob=1\n",
    "\n",
    "            \n",
    "            #Iterating through unqiue columns of input column.\n",
    "            for testcol in test_new_df.columns:\n",
    "                \n",
    "                \n",
    "                if row[testcol] >0:\n",
    "                    d=inp_out_prob[testcol]\n",
    "                    \n",
    "                \n",
    "                    p=d[testcol+testval] # column|class probabilty is obtained from model's dictionary of probabilites \n",
    "                                          # eg the|sport or the|nosport\n",
    "                                       \n",
    "                    testprob=testprob*p \n",
    "                    \n",
    "                #eg RednY * SportnY *domesticnY\n",
    "        \n",
    "        \n",
    "            testprob=testprob*class_dict[testval]\n",
    "            \n",
    "            finalprob.update({testval:testprob})\n",
    "        \n",
    "        max_v = max(zip(finalprob.values(), finalprob.keys()))  # find class having maximum probabilty\n",
    "    \n",
    "        Ypred.append(max_v[1]) # dictionary to populate prediction of a given row\n",
    "    return Ypred\n",
    "\n",
    "\n",
    "    \n",
    "def accuracyfunc(ypred,y_test):\n",
    "    \n",
    "    '''\n",
    "    The function is to calculate the accuracy score with predicted data and actual labels.\n",
    "    \n",
    "    Arguement :- \n",
    "    ypred :- Prediction of validation data\n",
    "    y_test :- Actual class of the data set\n",
    "    \n",
    "    Return :- \n",
    "    accuracy :- Accuracy score\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ytest=y_test.tolist()\n",
    "\n",
    "    ytestrow=len(ytest)\n",
    "\n",
    "\n",
    "    cntrow=0\n",
    "    accuracy=0\n",
    "    \n",
    "    for x,y in zip(ytest,ypred):\n",
    "        \n",
    "        if x==y:\n",
    "            cntrow=cntrow+1\n",
    "  \n",
    "    accuracy=cntrow*100/ytestrow\n",
    "    print(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data into data frame.\n",
    "df=read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split X data into training and testing data.\n",
    "x_train,x_test,y_train,y_test = data_preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranform training data sentence to count based value.\n",
    "x_train_transform,train_dist_word=count_vector(x_train,x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranform training data sentence to count based value.\n",
    "xtest_transform,train_dist_word=count_vector(x_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe with distinct words of training data as columns.\n",
    "new_df=create_newdf(x_train_transform,train_dist_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe with distinct words of test data as columns.\n",
    "testnewdf=create_newdf(xtest_transform,train_dist_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate probability of unique values of Y data.\n",
    "class_dict,unqout=class_prob(new_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the probability of distinct words | unique class of training data.\n",
    "inp_out_prob=prob_inp_out(train_dist_word,y_train,new_df,unqout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict test data.\n",
    "ypred=finalypred(testnewdf,inp_out_prob,unqout,class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.333333333333336\n"
     ]
    }
   ],
   "source": [
    "#Calculate accuracy score of predicted data and actual test Y data.\n",
    "accuracyfunc(ypred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
